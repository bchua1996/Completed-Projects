---
title: "ML Project"
author: "Brendan Chua"
date: "11/12/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

```{r}

library(dplyr)
library(ggplot2) # Load ggplot2 for visualizations
library(naniar) # Load nanair for missing data visualization
library(OneR) # Load OneR for binning function
library(mice) # Load mice for missing data inputation
library(glmnet)
library(plotmo)
```

```{r}
library(readr)
SimpleDataSetWithNewTarget <- read_csv("../Desktop/SimpleDataSetWithNewTarget.csv")
```

```{r}
summary(SimpleDataSetWithNewTarget)
str(SimpleDataSetWithNewTarget)
```

```{r}
head(SimpleDataSetWithNewTarget)
tail(SimpleDataSetWithNewTarget)
dim(SimpleDataSetWithNewTarget)
```
```{r}
MusicDatasetMSD <- SimpleDataSetWithNewTarget
colnames(MusicDatasetMSD)

str(MusicDatasetMSD)
```

#Rename variables
```{r}
MusicDatasetMSD <- MusicDatasetMSD %>% 
  rename(artist_hotness = artist_hotttnesss) %>%
  rename(song_hotness = song_hotttnesss) %>%
  rename(BillboardHot = bbhot)

```

#Visualize missing data 

```{r}
feature_variables <- names(MusicDatasetMSD)[] 
summary(MusicDatasetMSD[,feature_variables])
```
#I am interested in song_hotness, artist_hotness, and artist familiarity to conduct my initial analysis and visualisation. But I will want to visualize if there's any missing first. 

#Visualize missing data
```{r}
vis_miss(MusicDatasetMSD[, feature_variables]) 
```

#Checking missing data for song hotness
```{r}
summary(MusicDatasetMSD$BillboardHot[is.na(MusicDatasetMSD$song_hotness)])
summary(MusicDatasetMSD$BillboardHot[!is.na(MusicDatasetMSD$song_hotness)])
```

```{r}
t_bins <- bin(MusicDatasetMSD$BillboardHot, nbins = 2, method = "length") # Bin response variable
plot_dat <- cbind.data.frame(t_bins, MusicDatasetMSD[, feature_variables])
gg_miss_fct(x = plot_dat, fct = t_bins) +
  labs(x = "Billboard Hot100 ")
```
#There is too many missing data for song hotness, artist latitude, artist longitude, so I am going to drop those variables. 

#Drop artist latitude
```{r}
MusicDatasetMSD <- MusicDatasetMSD[,-(4:4)]  
```

#Drop artist longitude
```{r}
MusicDatasetMSD <- MusicDatasetMSD[,-(5:5)] 
```

#Drop song hotness
```{r}
MusicDatasetMSD <- MusicDatasetMSD[,-(14:14)] 
```

```{r}
feature_variables <- names(MusicDatasetMSD)[] 
summary(MusicDatasetMSD[,feature_variables])
```

#Visualizing missing data again to confirm if the variables are all good. 
```{r}
t_bins <- bin(MusicDatasetMSD$BillboardHot, nbins = 2, method = "length") # Bin response variable
plot_dat <- cbind.data.frame(t_bins, MusicDatasetMSD[, feature_variables])
gg_miss_fct(x = plot_dat, fct = t_bins) +
  labs(x = "Bill Board Top Music")
```

#Looks like artist location has too many missing values, so I am going to drop artist location as well.
```{r}
MusicDatasetMSD <- MusicDatasetMSD[,-(4:4)] 
```


```{r}
feature_variables <- names(MusicDatasetMSD)[] 
summary(MusicDatasetMSD[,feature_variables])
```

#The summary shows I am having 4 N/As with artist familiarity, but that is ok cause it is not a significant amount of missing values. For that, I decided to impute those 4 missing data for the artist familiarity variable
```{r}
set.seed(88888)
imputed_values <- mice( data = MusicDatasetMSD[, feature_variables], # Set dataset
                        m = 4, # Set number of multiple imputations
                        maxit = 40, # Set maximum number of iterations
                        method = "cart", # Set method
                        print = FALSE) # Set whether to print output or not
```

```{r}
MusicDatasetMSD[,feature_variables] <- complete(imputed_values, 1)
```

```{r}
summary(MusicDatasetMSD[feature_variables])
```

#Now everything looks good. I have a clean set of data that is ready for further analysis. 

#Time for some Viz. First I am going to plot artist familiarity agaisnt BillboardHot 
```{r}
g_1 <- ggplot(MusicDatasetMSD, aes(x = artist_familiarity, fill = factor(BillboardHot))) + 
  geom_density(alpha = 0.3) + # Use geom_density to get density plot
  theme_bw() + # Set theme for plot
  theme(panel.grid.major = element_blank(), # Turn of the background grid
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank()) +
  labs(x = "familiarity", # Set p,lot labels
       title = "Artist familiarity V BillboardHot appearance") +
  scale_fill_manual(values = c("0" = "red", "1" = "blue"), # Manually set fill values
    labels = c("0" = "No", "1" = "Yes"))
  
g_1
```

#The graphs shows it actually makes a lot of sense between artist familiarity and billboardhot appearance, with higher familiarity having a higher chance of appearing on the BillboardHot while lower artist familiarity resulted in a lower chance of appearing on the BillboardHot. 

#Next, I am checking on the relationship of plot artist hotness against BillboardHot
```{r}
g_2 <- ggplot(MusicDatasetMSD, aes(x = artist_hotness, fill = factor(BillboardHot))) + 
  geom_density(alpha = 0.3)+ 
  theme_bw() +
  theme(panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          panel.border = element_blank(),
          panel.background = element_blank()) +
  labs(x = 'artist hotness',
       title = 'Artist hotness V BillboardHot') +
   scale_fill_manual(values = c("0" = "red", "1" = "blue"), # Manually set fill values
    labels = c("0" = "No", "1" = "Yes"))

g_2
```

#Checking out some other variables 
```{r}
ggplot(MusicDatasetMSD, aes(x = loudness, fill = factor(BillboardHot))) + 
  geom_density(alpha = 0.3)+ 
  theme_bw() +
  theme(panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          panel.border = element_blank(),
          panel.background = element_blank()) +
  labs(x = 'loudness',
       title = 'loudness V BillboardHot') +
   scale_fill_manual(values = c("0" = "red", "1" = "blue"), # Manually set fill values
    labels = c("0" = "No", "1" = "Yes"))

ggplot(MusicDatasetMSD, aes(x = duration, fill = factor(BillboardHot))) + 
  geom_density(alpha = 0.3)+ 
  theme_bw() +
  theme(panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          panel.border = element_blank(),
          panel.background = element_blank()) +
  labs(x = 'duration',
       title = 'duration V BillboardHot') +
   scale_fill_manual(values = c("0" = "red", "1" = "blue"), # Manually set fill values
    labels = c("0" = "No", "1" = "Yes"))

ggplot(MusicDatasetMSD, aes(x = tempo, fill = factor(BillboardHot))) + 
  geom_density(alpha = 0.3)+ 
  theme_bw() +
  theme(panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          panel.border = element_blank(),
          panel.background = element_blank()) +
  labs(x = 'tempo',
       title = 'tempo V BillboardHot') +
   scale_fill_manual(values = c("0" = "red", "1" = "blue"), # Manually set fill values
    labels = c("0" = "No", "1" = "Yes"))

ggplot(MusicDatasetMSD, aes(x = time_signature, fill = factor(BillboardHot))) + 
  geom_density(alpha = 0.3)+ 
  theme_bw() +
  theme(panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          panel.border = element_blank(),
          panel.background = element_blank()) +
  labs(x = 'time_signature',
       title = 'time_signature V BillboardHot') +
   scale_fill_manual(values = c("0" = "red", "1" = "blue"), # Manually set fill values
    labels = c("0" = "No", "1" = "Yes"))

ggplot(MusicDatasetMSD, aes(x = key_confidence, fill = factor(BillboardHot))) + 
  geom_density(alpha = 0.3)+ 
  theme_bw() +
  theme(panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          panel.border = element_blank(),
          panel.background = element_blank()) +
  labs(x = 'key_confidence',
       title = 'key_confidence V BillboardHot') +
   scale_fill_manual(values = c("0" = "red", "1" = "blue"), # Manually set fill values
    labels = c("0" = "No", "1" = "Yes"))
```


#g_2 shows that artist hotness is a little skewed to the left, with the majority of it falling between the range of 0.3 and 0.45. An interesting finding from this plot is that artist hotness do have a role to play in terms of getting the song reaching the BillboardHot 100, with artist hotness above the range of 0.45 are more likely to have their song appearing in the Billboard Hot 100. 


#This concludes my initial finding for the short report. Artist familiarity and artist hotness do affect the chances of a song appearing in the BillboardHot 100 chart afterall. 

#Now, I would like to run an XGboost model for this project. But before that, I would need to clean some data and prepare the train and test dataset.


```{r}
str(MusicDatasetMSD)
```


```{r}
MusicDatasetMSD$artist_id <- as.numeric(MusicDatasetMSD$artist_id)
MusicDatasetMSD$artist_name <- as.numeric(MusicDatasetMSD$artist_name)
MusicDatasetMSD$release<- as.numeric(MusicDatasetMSD$release)
MusicDatasetMSD$title <- as.numeric(MusicDatasetMSD$title)
```

#Remove artist_id, artist_name, release, title cause not appropriate for numeric conversion 
```{r}
MusicDatasetMSD <- MusicDatasetMSD[, -(3:4)]
```

```{r}
MusicDatasetMSD <- MusicDatasetMSD[, -(10:10)]
```

```{r}
MusicDatasetMSD <- MusicDatasetMSD[, -(14:14)]
```

```{r}
summary(MusicDatasetMSD)
str(MusicDatasetMSD)
```


```{r}
library(xgboost) # Load XGBoost
library(caret) # Load Caret
library(OptimalCutpoints) # Load optimal cutpoints
library(ggplot2) # Load ggplot2
library(pROC) # Load proc
library(SHAPforxgboost) # Load shap for XGBoost
```

```{r}

library(splitstackshape)
set.seed(123456) # Set seed

#Perform stratified sampling
 split_dat <- stratified(MusicDatasetMSD, # Set dataset
                         group = "BillboardHot", # Set variables to use for stratification
                         size = 0.2,  # Set size of test set
                         bothSets = TRUE ) # Return both training and test sets
 # Extract train data
 train_dat <- split_dat[[2]]
 # Extract test data
 test_dat <- split_dat[[1]]

# Check size
nrow(train_dat)
nrow(test_dat)
```


```{r}
str(train_dat)
```

```{r}
sum(is.na(train_dat))
sum(is.na(test_dat))

```


```{r}
summary(as.factor(train_dat$BillboardHot))
summary(as.factor(test_dat$BillboardHot))
```

```{r}
# Create training matrix
dtrain <- xgb.DMatrix(data = as.matrix(train_dat[, 1:14]), label = as.numeric(train_dat$BillboardHot))
# Create test matrix
dtest <- xgb.DMatrix(data = as.matrix(test_dat[, 1:14]), label = as.numeric(test_dat$BillboardHot))
```


#Now that I have both train and test dataset, I would like to train the XGBoost model 
```{r}
set.seed(88888)
bst_1 <- xgboost(data = dtrain, # Set training data
               
               nrounds = 100, # Set number of rounds
               
               verbose = 1, # 1 - Prints out fit
                print_every_n = 20, # Prints out result every 20th iteration
               
               objective = "binary:logistic", # Set objective
               eval_metric = "auc",
               eval_metric = "error") # Set evaluation metric to use

```

#OK from the first run, I can see that when the Error Rate decreases as the AUC increases. 

#Now I would like to predict with XGboost 
```{r}
boost_preds <- predict(bst_1, dtrain) # Create predictions for xgboost model
# Join predictions and actual
pred_dat <- cbind.data.frame(boost_preds , train_dat$BillboardHot)
names(pred_dat) <- c("predictions", "response")
oc<- optimal.cutpoints(X = "predictions",
                       status = "response",
                       tag.healthy = 0,
                       data = pred_dat,
                       methods = "MaxEfficiency")

boost_preds_1 <- predict(bst_1, dtest) # Create predictions for xgboost model

pred_dat <- cbind.data.frame(boost_preds_1 , test_dat$BillboardHot)#
# Convert predictions to classes, using optimal cut-off
boost_pred_class <- rep(0, length(boost_preds_1))
boost_pred_class[boost_preds_1 >= oc$MaxEfficiency$Global$optimal.cutoff$cutoff[1]] <- 1


t <- table(boost_pred_class, test_dat$BillboardHot) # Create table
confusionMatrix(t, positive = "1") # Produce confusion matrix
```

#The first predicted result shows an accuracy rate here.

#Now I would like to tune my model to see how it fares. I am doing it with cross validation inside xgboost. 

```{r}
# Use xgb.cv to run cross-validation inside xgboost
set.seed(888888)
bst <- xgb.cv(data = dtrain, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
               eta = 0.1, # Set learning rate
              
               nrounds = 1000, # Set number of rounds
               early_stopping_rounds = 50, # Set number of rounds to stop at if there is no improvement
               
               verbose = 1, # 1 - Prints out fit
               nthread = 1, # Set number of parallel threads
               print_every_n = 20, # Prints out result every 20th iteration
              
               objective = "binary:logistic", # Set objective
               eval_metric = "auc",
               eval_metric = "error") # Set evaluation metric to use
```

#Next, I am tuning the max depth and min child weight 
```{r}
max_depth_vals <- c(3, 5, 7, 10, 15) # Create vector of max depth values
min_child_weight <- c(1,3,5,7, 10, 15) # Create vector of min child values

# Expand grid of parameter values
cv_params <- expand.grid(max_depth_vals, min_child_weight)
names(cv_params) <- c("max_depth", "min_child_weight")
# Create results vector
auc_vec <- error_vec <- rep(NA, nrow(cv_params)) 
# Loop through results
for(i in 1:nrow(cv_params)){
  set.seed(888888)
  bst_tune <- xgb.cv(data = dtrain, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.1, # Set learning rate
              max.depth = cv_params$max_depth[i], # Set max depth
              min_child_weight = cv_params$min_child_weight[i], # Set minimum number of samples in node to split
             
               
              nrounds = 100, # Set number of rounds
              early_stopping_rounds = 20, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
               
              objective = "binary:logistic", # Set objective
              eval_metric = "auc", # Set evaluation metric to use
              eval_metric = "error") # Set evaluation metric to use
  auc_vec[i] <- bst_tune$evaluation_log$test_auc_mean[bst_tune$best_ntreelimit]
  error_vec[i] <- bst_tune$evaluation_log$test_error_mean[bst_tune$best_ntreelimit]
  
}
```

#After getting the results, I am joinign them back to the dataset

```{r}
res_db <- cbind.data.frame(cv_params, auc_vec, error_vec)
names(res_db)[3:4] <- c("auc", "error")
```

#plotting a graph to see how it looks like. But before that I need to convert the max depth and min chld error to factor variables. 
```{r}
res_db$max_depth <- as.factor(res_db$max_depth)
res_db$min_child_weight <- as.factor(res_db$min_child_weight)
```

#OK now thre variables looks good and are ready for plotting. 

```{r}
g_3 <- ggplot(res_db, aes(y = max_depth, x = min_child_weight, fill = auc)) + # set aesthetics
  geom_tile() + # Use geom_tile for heatmap
  theme_bw() + # Set theme
  scale_fill_gradient2(low = "blue", # Choose low color
    mid = "white", # Choose mid color
    high = "red", # Choose high color
    midpoint =mean(res_db$auc), # Choose mid point
    space = "Lab", 
    na.value ="grey", # Choose NA value
    guide = "colourbar", # Set color bar
    aesthetics = "fill") + # Select aesthetics to apply
  labs(x = "Minimum Child Weight", y = "Max Depth", fill = "AUC") # Set labels
g_3 # Generate plot
```

#OK so without tuning, it looks like the best AUC happens when the Min child weight is at 10 and the Max depth is at 5. 

#printing error heat map 
```{r}
# print error heatmap
g_4 <- ggplot(res_db, aes(y = max_depth, x = min_child_weight, fill = error)) + # set aesthetics
  geom_tile() + # Use geom_tile for heatmap
  theme_bw() + # Set theme
  scale_fill_gradient2(low = "blue", # Choose low color
    mid = "white", # Choose mid color
    high = "red", # Choose high color
    midpoint =mean(res_db$error), # Choose mid point
    space = "Lab", 
    na.value ="grey", # Choose NA value
    guide = "colourbar", # Set color bar
    aesthetics = "fill") + # Select aesthetics to apply
  labs(x = "Minimum Child Weight", y = "Max Depth", fill = "Error") # Set labels
g_4 # Generate plot
```

#For the error, I have the best min child weight at between 5-15 and the max depth at 5. 

```{r}
res_db # Print results
```
#OK after interpreting the results, I found out that the best AUC and Error is when the max depth is at 5 and the min child weight is at 10. 

#Now I have a better model, I would like to improve it further. So I am tuning the gamma to determine the minimum loss reduction that is needed to add another partition in a node.

```{r}

gamma_vals <- c(0, 0.05, 0.1, 0.15, 0.2) # Create vector of gamma values

# Be Careful - This can take a very long time to run
set.seed(888888)
auc_vec <- error_vec <- rep(NA, length(gamma_vals))
for(i in 1:length(gamma_vals)){
  bst_tune <- xgb.cv(data = dtrain, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.1, # Set learning rate
              max.depth = 5, # Set max depth
              min_child_weight = 10, # Set minimum number of samples in node to split
              gamma = gamma_vals[i], # Set minimum loss reduction for split

              
               
              nrounds = 100, # Set number of rounds
              early_stopping_rounds = 20, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
               
              objective = "binary:logistic", # Set objective
              eval_metric = "auc", # Set evaluation metric to use
              eval_metric = "error") # Set evaluation metric to use
  auc_vec[i] <- bst_tune$evaluation_log$test_auc_mean[bst_tune$best_ntreelimit]
  error_vec[i] <- bst_tune$evaluation_log$test_error_mean[bst_tune$best_ntreelimit]
  
}
```

#See gamma results
```{r}
# Join gamma to values
cbind.data.frame(gamma_vals, auc_vec, error_vec)
```
#Great! AUC is the highest and error the lowest when GAMMA is at 0. And for that I will use GAMMA as 0 going forward. But will try to increaease the number of rounds to 1000 this time just to make it more useful to the model. 

```{r}
# Use xgb.cv to run cross-validation inside xgboost
set.seed(888888)
bst <- xgb.cv(data = dtrain, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.1, # Set learning rate
              max.depth = 5, # Set max depth
              min_child_weight = 10, # Set minimum number of samples in node to split
              gamma = 0, # Set minimum loss reduction for split
             
               
              nrounds = 1000, # Set number of rounds
              early_stopping_rounds = 50, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
               
              objective = "binary:logistic", # Set objective
              eval_metric = "auc",
              eval_metric = "error") # Set evaluation metric to use
```
#The results suggest I use 62 trees. 

#OK. Now I would like to tune the subsample and col_sample_by_tree parameters.
```{r}
# Be Careful - This can take a very long time to run
subsample <- c(0.6, 0.7, 0.8, 0.9, 1) # Create vector of subsample values
colsample_by_tree <- c(0.6, 0.7, 0.8, 0.9, 1) # Create vector of col sample values

# Expand grid of tuning parameters
cv_params <- expand.grid(subsample, colsample_by_tree)
names(cv_params) <- c("subsample", "colsample_by_tree")
# Create vectors to store results
auc_vec <- error_vec <- rep(NA, nrow(cv_params)) 
# Loop through parameter values
for(i in 1:nrow(cv_params)){
  set.seed(888888)
  bst_tune <- xgb.cv(data = dtrain, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.1, # Set learning rate
              max.depth = 5, # Set max depth
              min_child_weight = 10, # Set minimum number of samples in node to split
              gamma = 0, # Set minimum loss reduction for split
              subsample = cv_params$subsample[i], # Set proportion of training data to use in tree
              colsample_bytree = cv_params$colsample_by_tree[i], # Set number of variables to use in each tree
               
              nrounds = 150, # Set number of rounds
              early_stopping_rounds = 20, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
               
              objective = "binary:logistic", # Set objective
              eval_metric = "auc", # Set evaluation metric to use
              eval_metric = "error") # Set evaluation metric to use
  auc_vec[i] <- bst_tune$evaluation_log$test_auc_mean[bst_tune$best_ntreelimit]
  error_vec[i] <- bst_tune$evaluation_log$test_error_mean[bst_tune$best_ntreelimit]
  
}
```

#TIme to see how they look 

```{r}
res_db <- cbind.data.frame(cv_params, auc_vec, error_vec)
names(res_db)[3:4] <- c("auc", "error") 
res_db$subsample <- as.factor(res_db$subsample) # Convert tree number to factor for plotting
res_db$colsample_by_tree <- as.factor(res_db$colsample_by_tree) # Convert node size to factor for plotting
g_5 <- ggplot(res_db, aes(y = colsample_by_tree, x = subsample, fill = auc)) + # set aesthetics
  geom_tile() + # Use geom_tile for heatmap
  theme_bw() + # Set theme
  scale_fill_gradient2(low = "blue", # Choose low color
    mid = "white", # Choose mid color
    high = "red", # Choose high color
    midpoint =mean(res_db$auc), # Choose mid point
    space = "Lab", 
    na.value ="grey", # Choose NA value
    guide = "colourbar", # Set color bar
    aesthetics = "fill") + # Select aesthetics to apply
  labs(x = "Subsample", y = "Column Sample by Tree", fill = "AUC") # Set labels
g_5 # Generate plot
```


```{r}
g_6 <- ggplot(res_db, aes(y = colsample_by_tree, x = subsample, fill = error)) + # set aesthetics
  geom_tile() + # Use geom_tile for heatmap
  theme_bw() + # Set theme
  scale_fill_gradient2(low = "blue", # Choose low color
    mid = "white", # Choose mid color
    high = "red", # Choose high color
    midpoint =mean(res_db$error), # Choose mid point
    space = "Lab", 
    na.value ="grey", # Choose NA value
    guide = "colourbar", # Set color bar
    aesthetics = "fill") + # Select aesthetics to apply
  labs(x = "Subsample", y = "Column Sample by Tree", fill = "Error") # Set labels
g_6 # Generate plot
```

```{r}
res_db
```

#Well, the graph shows the best result is when the subsample is at 0.9 and then colsample_by_tree is at 0.8. Also, it looks like it does significantly better with more samples. It may mean that there are a bunch of variables that do not provide high magnitude of predictive power so those trees that were bullt with them are bad prodictors. 

#OK, now on to ETA tuning. 
#ETA to 0.3
```{r}
# Use xgb.cv to run cross-validation inside xgboost
set.seed(888888)
bst_mod_1 <- xgb.cv(data = dtrain, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.3, # Set learning rate
              max.depth = 5, # Set max depth
              min_child_weight = 10, # Set minimum number of samples in node to split
              gamma = 0, # Set minimum loss reduction for split
              subsample = 0.9, # Set proportion of training data to use in tree
              colsample_bytree =  0.8, # Set number of variables to use in each tree
               
              nrounds = 1000, # Set number of rounds
              early_stopping_rounds = 20, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              objective = "binary:logistic", # Set objective
              eval_metric = "auc",
              eval_metric = "error") # Set evaluation metric to use
```

#ETA to 0.1
```{r}
# Use xgb.cv to run cross-validation inside xgboost
set.seed(888888)
bst_mod_2 <- xgb.cv(data = dtrain, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.1, # Set learning rate
              max.depth = 5, # Set max depth
              min_child_weight = 10, # Set minimum number of samples in node to split
              gamma = 0, # Set minimum loss reduction for split
              subsample = 0.9, # Set proportion of training data to use in tree
              colsample_bytree =  0.8, # Set number of variables to use in each tree
               
              nrounds = 1000, # Set number of rounds
              early_stopping_rounds = 20, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              objective = "binary:logistic", # Set objective
              eval_metric = "auc",
              eval_metric = "error") # Set evaluation metric to use
```


#ETA to 0.05
```{r}
# Use xgb.cv to run cross-validation inside xgboost
set.seed(888888)
bst_mod_3 <- xgb.cv(data = dtrain, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.05, # Set learning rate
              max.depth = 5, # Set max depth
              min_child_weight = 10, # Set minimum number of samples in node to split
              gamma = 0, # Set minimum loss reduction for split
              subsample = 0.9, # Set proportion of training data to use in tree
              colsample_bytree =  0.8, # Set number of variables to use in each tree
               
              nrounds = 1000, # Set number of rounds
              early_stopping_rounds = 20, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              objective = "binary:logistic", # Set objective
              eval_metric = "auc",
              eval_metric = "error") # Set evaluation metric to use
```

#ETA to 0.01
```{r}
# Use xgb.cv to run cross-validation inside xgboost
set.seed(888888)
bst_mod_4 <- xgb.cv(data = dtrain, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.01, # Set learning rate
              max.depth = 5, # Set max depth
              min_child_weight = 10, # Set minimum number of samples in node to split
              gamma = 0, # Set minimum loss reduction for split
              subsample = 0.9, # Set proportion of training data to use in tree
              colsample_bytree =  0.8, # Set number of variables to use in each tree
               
              nrounds = 1000, # Set number of rounds
              early_stopping_rounds = 20, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              objective = "binary:logistic", # Set objective
              eval_metric = "auc",
              eval_metric = "error") # Set evaluation metric to use
```

#ETA to 0.005
```{r}
# Use xgb.cv to run cross-validation inside xgboost
set.seed(888888)
bst_mod_5 <- xgb.cv(data = dtrain, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.005, # Set learning rate
              max.depth = 5, # Set max depth
              min_child_weight = 10, # Set minimum number of samples in node to split
              gamma = 0, # Set minimum loss reduction for split
              subsample = 0.9, # Set proportion of training data to use in tree
              colsample_bytree =  0.8, # Set number of variables to use in each tree
               
              nrounds = 1000, # Set number of rounds
              early_stopping_rounds = 20, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              objective = "binary:logistic", # Set objective
              eval_metric = "auc",
              eval_metric = "error") # Set evaluation metric to use
```

#now that I have many those different ETAs, I would like to plot the error rates

```{r}
# Extract results for model with eta = 0.3
pd1 <- cbind.data.frame(bst_mod_1$evaluation_log[,c("iter", "test_error_mean")], rep(0.3, nrow(bst_mod_1$evaluation_log)))
names(pd1)[3] <- "eta"
# Extract results for model with eta = 0.1
pd2 <- cbind.data.frame(bst_mod_2$evaluation_log[,c("iter", "test_error_mean")], rep(0.1, nrow(bst_mod_2$evaluation_log)))
names(pd2)[3] <- "eta"
# Extract results for model with eta = 0.05
pd3 <- cbind.data.frame(bst_mod_3$evaluation_log[,c("iter", "test_error_mean")], rep(0.05, nrow(bst_mod_3$evaluation_log)))
names(pd3)[3] <- "eta"
# Extract results for model with eta = 0.01
pd4 <- cbind.data.frame(bst_mod_4$evaluation_log[,c("iter", "test_error_mean")], rep(0.01, nrow(bst_mod_4$evaluation_log)))
names(pd4)[3] <- "eta"
# Extract results for model with eta = 0.005
pd5 <- cbind.data.frame(bst_mod_5$evaluation_log[,c("iter", "test_error_mean")], rep(0.005, nrow(bst_mod_5$evaluation_log)))
names(pd5)[3] <- "eta"
# Join datasets
plot_data <- rbind.data.frame(pd1, pd2, pd3, pd4, pd5)
# Converty ETA to factor
plot_data$eta <- as.factor(plot_data$eta)
# Plot points
g_7 <- ggplot(plot_data, aes(x = iter, y = test_error_mean, color = eta))+
  geom_point(alpha = 0.5) +
  theme_bw() + # Set theme
  theme(panel.grid.major = element_blank(), # Remove grid
        panel.grid.minor = element_blank(), # Remove grid
        panel.border = element_blank(), # Remove grid
        panel.background = element_blank()) + # Remove grid 
  labs(x = "Number of Trees", title = "Error Rate v Number of Trees",
       y = "Error Rate", color = "Learning \n Rate")  # Set labels
g_7
```

```{r}
# Plot lines
g_8 <- ggplot(plot_data, aes(x = iter, y = test_error_mean, color = eta))+
  geom_smooth(alpha = 0.5) +
  theme_bw() + # Set theme
  theme(panel.grid.major = element_blank(), # Remove grid
        panel.grid.minor = element_blank(), # Remove grid
        panel.border = element_blank(), # Remove grid
        panel.background = element_blank()) + # Remove grid 
  labs(x = "Number of Trees", title = "Error Rate v Number of Trees",
       y = "Error Rate", color = "Learning \n Rate")  # Set labels
g_8
```

#I think this is an interesting plot. I would say the best is when ETA is at 0.1 . I am now going to do a final model. 

```{r}
set.seed(888888)
bst_final <- xgboost(data = dtrain, # Set training data
              
        
               
              eta = 0.1, # Set learning rate
              max.depth =  5, # Set max depth
              min_child_weight = 10, # Set minimum number of samples in node to split
              gamma = 0, # Set minimum loss reduction for split
              subsample =  0.9, # Set proportion of training data to use in tree
              colsample_bytree = 0.8, # Set number of variables to use in each tree
               
              nrounds = 1000, # Set number of rounds
              early_stopping_rounds = 20, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              objective = "binary:logistic", # Set objective
              eval_metric = "auc",
              eval_metric = "error") # Set evaluation metric to use
```


#OK now its time to see the model performance on the dataset

```{r}
library(OptimalCutpoints)

boost_preds_1 <- predict(bst_final, dtest) # Create predictions for xgboost model

# Convert predictions to classes, using optimal cut-off
boost_pred_class <- rep("0", length(boost_preds_1))
boost_pred_class[boost_preds_1 >= 0.5] <- "1"


t <- table(boost_pred_class, test_dat$BillboardHot) # Create table
confusionMatrix(t, positive = "1") # Produce confusion matrix
```
#The result shows the balanced accuracy when a cutoff point is at 0.5. But I would like to lower the cutoff rate to 0.1 to increase its usefulness. 

```{r}
boost_preds_1 <- predict(bst_final, dtest) # Create predictions for xgboost model

# Convert predictions to classes, using optimal cut-off
boost_pred_class <- rep("0", length(boost_preds_1))
boost_pred_class[boost_preds_1 >= 0.1] <- "1"


t <- table(boost_pred_class, test_dat$BillboardHot) # Create table
confusionMatrix(t, positive = "1") # Produce confusion matrix
```

#Now here I can see that the balanced accuracy rate has increased, which is good news!


#Cheking for imbalanced data
```{r}
summary(as.factor(train_dat$BillboardHot))
```

```{r}
zero_weight <- 954/7047 # Calculate proportion of positive samples in data
weight_vec <- rep(1, nrow(train_dat)) # Create weight vector
weight_vec[which(train_dat$BillboardHot == 0)] <- zero_weight # Assign weight for negative samples
sum(weight_vec[which(train_dat$BillboardHot == 1)]) # Check 1 class weight
sum(weight_vec[which(train_dat$BillboardHot == 0)])
```


```{r}
set.seed(888888)
bst_bal <- xgboost(data = dtrain, # Set training data
               
              eta = 0.1, # Set learning rate
              max.depth =  5, # Set max depth
              min_child_weight = 10, # Set minimum number of samples in node to split
              gamma = 0, # Set minimum loss reduction for split
              subsample =  0.9, # Set proportion of training data to use in tree
              colsample_bytree = 0.8, # Set number of variables to use in each tree
               
              nrounds = 1000, # Set number of rounds
              early_stopping_rounds = 20, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              weight = weight_vec, # Set weights
              
              objective = "binary:logistic", # Set objective
              eval_metric = "auc",
              eval_metric = "error") # Set evaluation metric to use
```

#Check for imbalanced data
```{r}
# Check results
set.seed(888888)
boost_preds_bal <- predict(bst_bal, dtrain) # Create predictions for XGBoost model on training data

pred_dat <- cbind.data.frame(boost_preds_bal , train_dat$BillboardHot)#
names(pred_dat) <- c("predictions", "response")
oc<- optimal.cutpoints(X = "predictions",
                       status = "response",
                       tag.healthy = 0,
                       data = pred_dat,
                       methods = "MaxEfficiency")

boost_preds_bal <- predict(bst_bal, dtest) # Create predictions for XGBoost model

pred_dat <- cbind.data.frame(boost_preds_bal , test_dat$BillboardHot)#
# Convert predictions to classes, using optimal cut-off
boost_pred_class <- rep(0, length(boost_preds_bal))
boost_pred_class[boost_preds_bal >= oc$MaxEfficiency$Global$optimal.cutoff$cutoff[1]] <- 1


t <- table(boost_pred_class, test_dat$BillboardHot) # Create table
confusionMatrix(t, positive = "1") # Produce confusion matrix
```


#The results shows a slightly lower balanced accuracy rate, but thats ok, I will take that. 

#Now, I woud like to determine the variable importance

```{r}
shap.score.rank <- function(xgb_model = xgb_mod, shap_approx = TRUE, 
                            X_train = mydata$train_mm){
  require(xgboost)
  require(data.table)
  shap_contrib <- predict(xgb_model, X_train,
                          predcontrib = TRUE, approxcontrib = shap_approx)
  shap_contrib <- as.data.table(shap_contrib)
  shap_contrib[,BIAS:=NULL]
  cat('make SHAP score by decreasing order\n\n')
  mean_shap_score <- colMeans(abs(shap_contrib))[order(colMeans(abs(shap_contrib)), decreasing = T)]
  return(list(shap_score = shap_contrib,
              mean_shap_score = (mean_shap_score)))
}

# a function to standardize feature values into same range
std1 <- function(x){
  return ((x - min(x, na.rm = T))/(max(x, na.rm = T) - min(x, na.rm = T)))
}


# prep shap data
shap.prep <- function(shap  = shap_result, X_train = mydata$train_mm, top_n){
  require(ggforce)
  # descending order
  if (missing(top_n)) top_n <- dim(X_train)[2] # by default, use all features
  if (!top_n%in%c(1:dim(X_train)[2])) stop('supply correct top_n')
  require(data.table)
  shap_score_sub <- as.data.table(shap$shap_score)
  shap_score_sub <- shap_score_sub[, names(shap$mean_shap_score)[1:top_n], with = F]
  shap_score_long <- melt.data.table(shap_score_sub, measure.vars = colnames(shap_score_sub))
  
  # feature values: the values in the original dataset
  fv_sub <- as.data.table(X_train)[, names(shap$mean_shap_score)[1:top_n], with = F]
  # standardize feature values
  fv_sub_long <- melt.data.table(fv_sub, measure.vars = colnames(fv_sub))
  fv_sub_long[, stdfvalue := std1(value), by = "variable"]
  # SHAP value: value
  # raw feature value: rfvalue; 
  # standarized: stdfvalue
  names(fv_sub_long) <- c("variable", "rfvalue", "stdfvalue" )
  shap_long2 <- cbind(shap_score_long, fv_sub_long[,c('rfvalue','stdfvalue')])
  shap_long2[, mean_value := mean(abs(value)), by = variable]
  setkey(shap_long2, variable)
  return(shap_long2) 
}

plot.shap.summary <- function(data_long){
  x_bound <- max(abs(data_long$value))
  require('ggforce') # for `geom_sina`
  plot1 <- ggplot(data = data_long)+
    coord_flip() + 
    # sina plot: 
    geom_sina(aes(x = variable, y = value, color = stdfvalue)) +
    # print the mean absolute value: 
    geom_text(data = unique(data_long[, c("variable", "mean_value"), with = F]),
              aes(x = variable, y=-Inf, label = sprintf("%.3f", mean_value)),
              size = 3, alpha = 0.7,
              hjust = -0.2, 
              fontface = "bold") + # bold
    # # add a "SHAP" bar notation
    # annotate("text", x = -Inf, y = -Inf, vjust = -0.2, hjust = 0, size = 3,
    #          label = expression(group("|", bar(SHAP), "|"))) + 
    scale_color_gradient(low="#FFCC33", high="#6600CC", 
                         breaks=c(0,1), labels=c("Low","High")) +
    theme_bw() + 
    theme(axis.line.y = element_blank(), axis.ticks.y = element_blank(), # remove axis line
          legend.position="bottom") + 
    geom_hline(yintercept = 0) + # the vertical line
    scale_y_continuous(limits = c(-x_bound, x_bound)) +
    # reverse the order of features
    scale_x_discrete(limits = rev(levels(data_long$variable)) 
    ) + 
    labs(y = "SHAP value (impact on model output)", x = "", color = "Feature value") 
  return(plot1)
}

var_importance <- function(shap_result, top_n=10)
{
  var_importance=tibble(var=names(shap_result$mean_shap_score), importance=shap_result$mean_shap_score)
  
  var_importance=var_importance[1:top_n,]
  
  ggplot(var_importance, aes(x=reorder(var,importance), y=importance)) + 
    geom_bar(stat = "identity") + 
    coord_flip() + 
    theme_light() + 
    theme(axis.title.y=element_blank()) 
}
```

```{r}
# Extract importance
imp_mat <- xgb.importance(model = bst_final)
# Plot importance (top 10 variables)
xgb.plot.importance(imp_mat, top_n = 10)
```

#Hmm.. seems like artist hotness is the most important variableaccording to XGBoost. 

#Trying to run a SHAP to see the affect of each variable

```{r}
x_vars <- model.matrix(MusicDatasetMSD$BillboardHot ~., data = MusicDatasetMSD[,1:15])[,-1]
# Create DMatrix
dtrain <- xgb.DMatrix(data =x_vars, label = MusicDatasetMSD$BillboardHot)
# Fit XGBoost
bst_SHAP <- xgboost(data = dtrain, # Set training data
                    nrounds = 100, # Set number of rounds
                    verbose = 1, # 1 - Prints out fit
                    print_every_n = 20, # Prints out result every 20th iteration
                    objective = "binary:logistic", # Set objective
                    eval_metric = "auc",
                    eval_metric = "error") # Set evaluation metric to use
```

```{r}
shap_values <- predict(bst_final,
                    x_vars,
                    predcontrib = TRUE,
                    approxcontrib = F)

shap_values[1,]
```

#So from the SHAP values, I can see that the artist hotness does have the highest SHAP values 
```{r}
# Extract standard importance
imp_mat_SHAP <- xgb.importance(model = bst_final)
# Plot standard importance (top 10 variables)
xgb.plot.importance(imp_mat_SHAP, top_n = 10)
```

#With XGBoost. Artist hotness is the most imortant variable. 

```{r}

library(ggforce)
library(pROC)
shap_result <- shap.score.rank(xgb_model = bst_final,
                X_train =as.matrix(train_dat[, 1:14]),
                shap_approx = F)
```

```{r}
var_importance(shap_result, top_n=10)

```

#With SHAP, duration (surprisingly) is the most important variable, leading with a wide margin to the second most important variable. I find this to be pretty interesting as I will ever thought of duration as the most important factor. 

```{r}
shap_long = shap.prep(shap = shap_result,
                           X_train = as.matrix(train_dat[, 1:14]),
                           top_n = 10)


plot.shap.summary(data_long = shap_long)
```

#The SHAP value plot suggest that duration does infact have the highest impact on model output, even though its feature value is not the highest . On the other hand, artist hotness stretches furthest to the highest level of the feature value but came in third overall. 

#Time signature plot 
```{r}
# Count results
temp <- as.data.frame(count(MusicDatasetMSD, time_signature, BillboardHot))
# Create transperancy

#temp$alpha_vec <- c(0.7, 1, 0.7, 0.5)
# Create plot
g_9 <- ggplot(temp, aes(x = time_signature, fill = factor(BillboardHot), y = n)) + #alpha = alpha_vec)) +
  geom_bar(position = "dodge", stat = "identity") +
  theme(panel.grid.major = element_blank(), # Remove grid
        panel.grid.minor = element_blank(), # Remove grid
        panel.border = element_blank(), # Remove grid
        panel.background = element_blank()) +
  scale_fill_manual(values = c("1" = "blue", "0" = "lightgrey"),
                    labels = c("1" = "Succesful", "0" = "Unsuccesful")) +
  labs(x = "time signature", y = "Count", fill = "Billboard \n Result",
       title = "Does Time Signature Affect Billboard Hot100 Chances?")  +
  annotate("text", x = 1.3, y = 400, label = "13.2%", size = 3) +
  annotate("text", x = 3.3, y = 400, label = "12.9%", size = 3) +
  annotate("text", x = 4.3, y = 1000, label = "14.0%", size = 3) +
  annotate("text", x = 5.3, y = 300, label = "11.1%", size = 3) +
  annotate("text", x = 7.3, y = 200, label = "12.4%", size = 3) +
  guides(alpha = "none")
g_9
```

#The time signature plot proved that if does affect the chances of a song's success, with the highest probability at 4/4. 

#Mode plot
```{r}
# Count results
temp_2 <- as.data.frame(count(MusicDatasetMSD, mode, BillboardHot))
# Create transperancy

temp_2<- temp_2 %>%
  mutate(`mode` = replace(`mode`, `mode` == '1', 'major key'))%>%
  mutate(`mode` = replace(`mode`, `mode` == '0', 'minor key')) 
#temp$alpha_vec <- c(0.7, 1, 0.7, 0.5)
# Create plot
g_10 <- ggplot(temp_2, aes(x = mode, fill = factor(BillboardHot), y = n ))+ #, alpha = 0.9)) +
  geom_bar(position = "dodge", stat = "identity") +
  theme(panel.grid.major = element_blank(), # Remove grid
        panel.grid.minor = element_blank(), # Remove grid
        panel.border = element_blank(), # Remove grid
        panel.background = element_blank()) +
  scale_fill_manual(values = c("1" = "blue", "0" = "lightgrey"),
                    labels = c("1" = "Succesful", "0" = "Unsuccesful")) +
  labs(x = "mode", y = "Count", fill = "Billboard \n Result",
         title = "Does Mode (a.k.a. major/minor key) Affect Billboard Hot100 Chances?") +
  annotate("text", x = 1.2, y = 1200, label = "14.29%", size = 5) +
  annotate("text", x = 2.2, y = 800, label = "11.88%", size = 5) +

  guides(alpha = "none")
g_10
```

#The mode plot suggest that songwriters write songs in major keys to have a higher chance of success

#Key plot 
```{r}
# Count results
temp_3 <- as.data.frame(count(MusicDatasetMSD, key, BillboardHot))
# Create transperancy
library(tidyverse)
library(dplyr)
temp_3 <- temp_3 %>%
  mutate(`key` = replace(`key`, `key` == '0', 'C')) %>%
  mutate(`key` = replace(`key`, `key` == '1', 'C#/Db')) %>%
  mutate(`key` = replace(`key`, `key` == '2', 'D')) %>%
  mutate(`key` = replace(`key`, `key` == '3', 'D#/Eb')) %>%
  mutate(`key` = replace(`key`, `key` == '4', 'E')) %>%
  mutate(`key` = replace(`key`, `key` == '5', 'F')) %>%
  mutate(`key` = replace(`key`, `key` == '6', 'F#/Gb')) %>%
  mutate(`key` = replace(`key`, `key` == '7', 'G')) %>%
  mutate(`key` = replace(`key`, `key` == '8', 'G#/Ab')) %>%
  mutate(`key` = replace(`key`, `key` == '9', 'A')) %>%
  mutate(`key` = replace(`key`, `key` == '10', 'A#/Bb')) %>%
  mutate(`key` = replace(`key`, `key` == '11', 'B'))
#temp$alpha_vec <- c(0.7, 1, 0.7, 0.5)
# Create plot
g_11 <- ggplot(temp_3, aes(x = key, fill = factor(BillboardHot), y = n)) + #alpha = alpha_vec)) +
  geom_bar(position = "dodge", stat = "identity") +
  theme(panel.grid.major = element_blank(), # Remove grid
        panel.grid.minor = element_blank(), # Remove grid
        panel.border = element_blank(), # Remove grid
        panel.background = element_blank(),
        panel.grid.major.y = element_blank()) +
  scale_fill_manual(values = c("1" = "blue", "0" = "lightgrey"),
                    labels = c("1" = "Succesful", "0" = "Unsuccesful")) +
  labs(x = "key", y = "Count", fill = "Billboard \n Result",
       title = "Does Song Key Affect Billboard Hot100 Chances?")  +
  annotate("text", x = 1.2, y = 180, label = "16.2%", size = 3) +
  annotate("text", x = 2.2, y = 150, label = "14.9%", size = 3) +
  annotate("text", x = 12.2, y = 100, label = "11.6%", size = 3) +
  annotate("text", x = 3.2, y = 150, label = "13.1%", size = 3) +
  annotate("text", x = 4.2, y = 230, label = "12.9%", size = 3) +
  annotate("text", x = 5.2, y = 150, label = "13.7%", size = 3) +
  annotate("text", x = 6.2, y = 170, label = "13.1%", size = 3) +
  annotate("text", x = 7.2, y = 80, label = "12.2%", size = 3) +
  annotate("text", x = 8.2, y = 140, label = "11.1%", size = 3) +
  annotate("text", x = 9.2, y = 140, label = "12.7%", size = 3) +
  annotate("text", x = 10.2, y = 130, label = "15.3%", size = 3) +
  annotate("text", x = 11.2, y = 200, label = "15.3%", size = 3) +
  guides(alpha = "none")+
  coord_flip()
g_11
```

#The key plot suggest that a song written in the key of 'A' has the highest chance of being successful. 

